<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</title>
    <!-- Bootstrap -->
    <link href="./files/bootstrap-4.4.1.css" rel="stylesheet">
    <link rel="stylesheet" href="./files/font-awesome.min.css">
</head>

<!-- cover -->

<body>
    <section>
        <div class="jumbotron text-center mt-4">
            <div class="container">
                <div class="row">
                    <div class="col-12">
                        <h2>AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis
                        </h2>
                        <!-- <hr> -->
                        <h6><a href="https://yudongguo.github.io/" target="_blank">Yudong Guo</a><sup>1,2</sup>
                            &nbsp;&nbsp;&nbsp;&nbsp;
                            <a href="http://kychern.github.io/" target="_blank">Keyu Chen</a><sup>1,2</sup>
                            &nbsp;&nbsp;&nbsp;&nbsp;
                            <a href="https://scholar.google.com/citations?user=Yv_olnAAAAAJ&hl" target="_blank">Sen
                                Liang</a><sup>3</sup>
                            &nbsp;&nbsp;&nbsp;&nbsp;
                            <a href="https://cg.cs.tsinghua.edu.cn/people/~Yongjin/Yongjin.htm"
                                target="_blank">Yongjin-Liu</a><sup>4</sup>
                            &nbsp;&nbsp;&nbsp;&nbsp;
                            <a href="http://www.cad.zju.edu.cn/bao/" target="_blank">Hujun Bao</a><sup>3</sup>
                            &nbsp;&nbsp;&nbsp;&nbsp;
                            <a href="http://staff.ustc.edu.cn/~juyong/" target="_blank">Juyong Zhang</a><sup>1</sup>
                            </p>
                            <sup>1</sup>University of Science and Technology of China
                            &nbsp;&nbsp;&nbsp;
                            <sup>2</sup>Beijing Dilusense Technology Corporation
                            <p>
                                <sup>3</sup>Zhejiang University
                                &nbsp;&nbsp;&nbsp;
                                <sup>4</sup>Tsinghua University
                            <ul class="nav nav-pills nav-justified">
                                <li>
                                    <a href="https://arxiv.org/abs/2103.11078">
                                        <image src="image/paper_image.png" height="60px">
                                            <h4><strong>Paper</strong></h4>
                                    </a>
                                </li>
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <li>
                                    <a href="https://www.youtube.com/watch?v=TQO2EBYXLyU">
                                        <image src="image/youtube_icon.png" height="60px">
                                            <h4><strong>Video</strong></h4>
                                    </a>
                                </li>
                            </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- <div class="row">
        <div class="col-md-4 col-md-offset-4 text-center">
            
        </div>
    </div> -->


    <section>
        <div class="container">
            <h3><b>Introduction</b></h3>
            <div class="row">
                <div class="col-12 text-center">
                    <p class="text-justify">
                        Generating high-fidelity talking head video by fitting with the input audio sequence is a
                        challenging problem that receives considerable attentions recently. In this paper, we
                        address this
                        problem with the aid of neural scene representation networks. Our method is completely
                        different
                        from existing methods that rely on intermediate representations like 2D landmarks or 3D face
                        models
                        to bridge the gap between audio input and video output. Specifically, the feature of input
                        audio
                        signal is directly fed into a conditional implicit function to generate a dynamic neural
                        radiance
                        field, from which a high-fidelity talking-head video corresponding to the audio signal is
                        synthesized using volume rendering. Another advantage of our framework is that not only the
                        head
                        (with hair) region is synthesized as previous methods did, but also the upper body is
                        generated via
                        two individual neural radiance fields. Experimental results demonstrate that our novel
                        framework can
                        (1) produce high-fidelity and natural results, and (2) support free adjustment of audio
                        signals,
                        viewing directions, and background images.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12 text-center" id="pipeline">
                    <h3>Pipeline</h3>
                    <video width="1100" height="372" src="video/pipeline.mp4" allowfullscreen controls></video>
                    <p class="text-justify">
                        Our talking-head synthesis framework is trained
                        on a short video sequence along with the audio track of a
                        target person. Based on the neural rendering idea, we implicitly model the deformed human heads
                        and upper bodies
                        by neural scene representation, i.e., neural radiance fields.
                        In order to bridge the domain gap between audio signals
                        and visual faces, we extract the semantic audio features and
                        learn a conditional implicit function to map the audio features to neural radiance fields.
                        Finally, visual
                        faces are rendered from the neural radiance fields using volumetric rendering.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12 text-center" id="audio_driven">
                    <h3>Audio Driven Results</h3>
                    <video width="1100" height="496" src="video/audio_driven.mp4" controls allowfullscreen></video>
                    <p class="text-justify">
                        Our method allows arbitrary audio input from different
                        identity, gender and language to drive target persons.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12 text-center" id="audio_driven">
                    <h3>Background & Pose Editing</h3>
                    <!-- <hr style="margin-top:0px"> -->
                    <video width="1100" height="529" src="video/application.mp4" allowfullscreen controls></video>
                    <p class="text-justify">
                        Our method can generate talking head frames with freely adjusted viewing directions and
                        various background images.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12 text-center" id="audio_driven">
                    <h3>Individual NeRFs Representation</h3>
                    <!-- <hr style="margin-top:0px"> -->
                    <video width="1100" height="426" src="video/head_torso.mp4" allowfullscreen controls></video>
                    <p class="text-justify">
                        We decompose the neural radiance fields of human
                        portrait scenes into two branches to model the head and
                        torso deformation respectively, which helps to generate more natural talking head results.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section>
        <div class="container">
            <div class="row">
                <div class="col-12 text-center" id="audio_driven">
                    <h3>Comparisons</h3>
                    <!-- <hr style="margin-top:0px"> -->
                    <video width="1100" height="618" src="video/comparison.mp4" allowfullscreen controls></video>
                    <p class="text-justify">
                        The image-based talking head methods are restricted by the input image size and thus could not
                        producing high-resolution imagery as we do. The model-based methods generally require large
                        quantities of training data. Moreover, our method owns the advantage of freely
                        manipulating the viewing directions on the target person, which means that we can naturally
                        rotate the “virtual camera” to observe the speaking actors from arbitrary novel angles.
                    </p>
                </div>
            </div>
        </div>
    </section>

    </p>
    <!-- citing -->
    <div class="container">
        <div class="row ">
            <div class="col-12">
                <h3>Citation</h3>
                <pre style="background-color: #e9eeef;padding: 1.25em 1.5em"><code>@misc{guo2021adnerf,
    title={AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis},
    author={Yudong Guo and Keyu Chen and Sen Liang and Yongjin Liu and Hujun Bao and Juyong Zhang},
    year={2021},
    eprint={2103.11078},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}</code></pre>
                <hr>
            </div>
        </div>
    </div>

</body>

</html>